/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO:swift] Successfully registered `/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Setting args.remove_unused_columns: False
[INFO:swift] rank: 0, local_rank: 0, world_size: 1, local_world_size: 1
[INFO:swift.hub.hub] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-7B-Instruct
[INFO:modelscope] Target directory already exists, skipping creation.
[INFO:swift] Loading the model using model_dir: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct
[INFO:swift] Because len(args.val_dataset) > 0, setting split_dataset_ratio: 0.0
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] output_dir: output/v15-20260210-074724
[INFO:swift] Global seed set to 5
[INFO:swift] args: RLHFArguments(
_n_gpu=-1,
acc_steps=1,
acc_strategy=token,
accelerator_config={'dispatch_batches': False},
adafactor=False,
adalora_beta1=0.85,
adalora_beta2=0.85,
adalora_deltaT=1,
adalora_init_r=12,
adalora_orth_reg_weight=0.5,
adalora_target_r=8,
adalora_tfinal=0,
adalora_tinit=0,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
adapter_act=gelu,
adapter_length=128,
adapters=[],
add_version=True,
async_generate=False,
attn_impl=None,
auto_find_batch_size=False,
average_tokens_across_devices=True,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_storage=None,
bnb_4bit_quant_type=nf4,
bnb_4bit_use_double_quant=True,
boft_block_num=0,
boft_block_size=4,
boft_dropout=0.0,
boft_n_butterfly_factor=1,
check_model=True,
ckpt_dir=None,
cliprange=0.2,
cliprange_value=0.2,
columns={},
cosine_max_len=None,
cosine_max_len_value_correct=0.5,
cosine_max_len_value_wrong=-0.5,
cosine_min_len_value_correct=1.0,
cosine_min_len_value_wrong=0.0,
cpo_alpha=1.0,
create_checkpoint_symlink=False,
custom_dataset_info=[],
custom_register_path=[],
data_seed=5,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset=['datasets/original/mr_train.jsonl'],
dataset_num_proc=4,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=None,
deepspeed=None,
desirable_weight=1.0,
device_map=None,
disable_tqdm=None,
do_eval=False,
do_predict=False,
do_train=False,
download_mode=reuse_dataset_if_exists,
ds3_gather_for_generation=True,
enable_cache=False,
epsilon=0.2,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=100.0,
eval_strategy=steps,
eval_use_gather_object=False,
external_plugins=[],
fourier_n_frequency=2000,
fourier_scaling=300.0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_aligner=True,
freeze_llm=False,
freeze_parameters=[],
freeze_parameters_ratio=0.0,
freeze_vit=True,
fsdp=None,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_num=1,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
galore_cos_threshold=0.4,
galore_gamma_proj=2,
galore_optim_per_parameter=False,
galore_proj_bits=4,
galore_proj_group_size=256,
galore_proj_quant=False,
galore_proj_type=std,
galore_quantization=False,
galore_queue_size=5,
galore_rank=128,
galore_scale=1.0,
galore_target_modules=None,
galore_update_proj_gap=50,
galore_with_embedding=False,
gamma=1.0,
gc_collect_after_offload=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hqq_axis=None,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_args_error=False,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_weights=True,
jit_mode_eval=False,
kl_coef=0.05,
label_names=None,
label_smoothing=0,
label_smoothing_factor=0.0,
lam=0.95,
lazy_tokenize=False,
learning_rate=1e-06,
length_column_name=length,
liger_kernel_config=None,
lisa_activated_layers=0,
lisa_step_interval=20,
llamapro_num_groups=None,
llamapro_num_new_blocks=4,
lmdeploy_cache_max_entry_count=0.8,
lmdeploy_device=auto,
lmdeploy_session_len=None,
load_args=False,
load_best_model_at_end=False,
load_data_args=False,
load_dataset_config=None,
local_rank=0,
local_repo_path=None,
local_rollout_forward_batch_size=64,
log_completions=True,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/output/v15-20260210-074724/runs,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
logprobs=False,
lora_alpha=32,
lora_bias=none,
lora_dropout=0.05,
lora_dtype=None,
lora_ga_batch_size=2,
lora_ga_direction=ArB2r,
lora_ga_iters=2,
lora_ga_max_length=1024,
lora_ga_scale=stable,
lora_ga_stable_gamma=16,
lora_modules=[],
lora_rank=8,
lorap_lr_ratio=None,
loss_scale=last_round,
loss_type=None,
lr_scheduler_kwargs=None,
lr_scheduler_type=cosine,
max_completion_length=1024,
max_grad_norm=1.0,
max_length=2048,
max_memory={},
max_new_tokens=64,
max_pixels=None,
max_steps=-1,
metric=None,
metric_for_best_model=reward,
metric_warmup_step=0,
missing_eos_penalty=None,
model=Qwen/Qwen2.5-7B-Instruct,
model_author=[None, None],
model_kwargs={},
model_layer_cls_name=None,
model_name=[None, None],
model_revision=None,
model_type=qwen2_5,
modules_to_save=[],
move_model_batches=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
norm_bbox=None,
num_beams=1,
num_generations=4,
num_infer_workers=1,
num_iterations=1,
num_labels=None,
num_mini_batches=1,
num_ppo_epochs=4,
num_sample_generations=10,
num_train_epochs=10.0,
offload_model=False,
offload_optimizer=False,
optim=adamw_torch_fused,
optim_args=None,
optim_target_modules=None,
optimizer=None,
output_dir=/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/output/v15-20260210-074724,
overwrite_output_dir=False,
packing=False,
padding_side=right,
parallelism_config=None,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
project=huggingface,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_bits=None,
quant_method=None,
ray_scope=last,
ref_model=None,
ref_model_revision=None,
ref_model_type=None,
reft_args=None,
reft_intervention_type=LoreftIntervention,
reft_layer_key=None,
reft_layers=None,
reft_rank=4,
remove_unused_columns=False,
repetition_max_penalty=-1.0,
repetition_n_grams=3,
repetition_penalty=1.0,
report_to=['wandb'],
response_length=512,
response_prefix=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
resume_only_model=False,
reward_adapters=[],
reward_funcs=['accuracy', 'format'],
reward_model=None,
reward_model_revision=None,
reward_model_type=None,
reward_weights=None,
rlhf_type=grpo,
rope_scaling=None,
rpo_alpha=1.0,
run_name=None,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100.0,
save_strategy=steps,
save_total_limit=20,
seed=5,
sequence_parallel_size=1,
simpo_gamma=1,
skip_memory_metrics=True,
sleep_level=0,
sortish_sampler=False,
split_dataset_ratio=0.0,
stop_words=[],
stream=False,
streaming=False,
strict=False,
swanlab_exp_name=None,
swanlab_mode=cloud,
swanlab_project=None,
swanlab_token=<SWANLAB_TOKEN>,
swanlab_workspace=None,
system=A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>,
target_modules=['all-linear'],
target_regex=None,
task_type=causal_lm,
temperature=0.9,
template=qwen2_5,
template_backend=swift,
tensor_parallel_size=1,
tf32=None,
tools_prompt=react_en,
top_k=50,
top_logprobs=None,
top_p=0.9,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=torch.bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
trackio_space_id=trackio,
train_type=lora,
trainable_parameters=[],
truncation_strategy=left,
tuner_backend=peft,
undesirable_weight=1.0,
use_chat_template=True,
use_cpu=False,
use_dora=False,
use_galore=False,
use_hf=False,
use_legacy_prediction_loop=False,
use_liger=False,
use_liger_kernel=False,
use_lmdeploy=False,
use_mps_device=False,
use_rslora=False,
use_swift_lora=False,
use_vllm=True,
val_dataset=['datasets/original/mr_val.jsonl'],
vera_d_initial=0.1,
vera_dropout=0.0,
vera_projection_prng_key=0,
vera_rank=256,
vf_coef=0.1,
vllm_device=['auto'],
vllm_enable_prefix_caching=True,
vllm_enforce_eager=False,
vllm_gpu_memory_utilization=0.5,
vllm_limit_mm_per_prompt=None,
vllm_max_model_len=None,
vllm_max_num_seqs=256,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.1,
whiten_rewards=False,
zero_hpz_partition_size=None,
)
[INFO:swift.hub.hub] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-7B-Instruct
[INFO:modelscope] Target directory already exists, skipping creation.
[INFO:swift] Loading the model using model_dir: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.22s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.15s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.05s/it]
[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}
[INFO:swift] model_info: ModelInfo(model_type='qwen2_5', model_dir='/qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct', torch_dtype=torch.bfloat16, max_model_len=32768, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.6",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}
, task_type='causal_lm', num_labels=None)
[INFO:swift] model.generation_config: GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "max_new_tokens": 64,
  "pad_token_id": 151643,
  "temperature": 0.9,
  "top_p": 0.9
}

[INFO:swift] default_system: A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>
[INFO:swift] The RLHFArguments will be saved in: /qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/output/v15-20260210-074724/args.json
[INFO:swift] Start time of running main: 2026-02-10 07:47:42.724177
[INFO:swift] create tmp_dir: /qiuyeqing/.cache/modelscope/tmp/hf_datasets-zh4lwk6d

Map (num_proc=4):   0%|          | 0/8462 [00:00<?, ? examples/s]
Map (num_proc=4):  25%|██▌       | 2116/8462 [00:00<00:00, 13479.36 examples/s]
Map (num_proc=4): 100%|██████████| 8462/8462 [00:00<00:00, 27196.71 examples/s]

Map (num_proc=4):   0%|          | 0/8462 [00:00<?, ? examples/s]
Map (num_proc=4):  12%|█▏        | 1000/8462 [00:00<00:01, 4628.12 examples/s]
Map (num_proc=4): 100%|██████████| 8462/8462 [00:00<00:00, 20051.85 examples/s]

Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]
Map (num_proc=4):  25%|██▌       | 50/200 [00:00<00:00, 295.11 examples/s]
Map (num_proc=4): 100%|██████████| 200/200 [00:00<00:00, 580.33 examples/s]

Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]
Map (num_proc=4):  25%|██▌       | 50/200 [00:00<00:00, 276.65 examples/s]
Map (num_proc=4): 100%|██████████| 200/200 [00:00<00:00, 555.37 examples/s]
[INFO:swift] train_dataset: Dataset({
    features: ['messages', 'solution'],
    num_rows: 8462
})
[INFO:swift] val_dataset: Dataset({
    features: ['messages', 'solution'],
    num_rows: 200
})
[INFO:swift] The split dataset from the training set will be saved at: /qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/output/v15-20260210-074724/val_dataset.jsonl.
[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct', revision=None, inference_mode=False, r=8, target_modules={'gate_proj', 'up_proj', 'v_proj', 'q_proj', 'k_proj', 'down_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)
[INFO:swift] model: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(152064, 3584)
        (layers): ModuleList(
          (0-27): 28 x Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3584, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=512, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=512, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=512, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=512, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3584, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): Qwen2MLP(
              (gate_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=18944, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear(
                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3584, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=18944, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=18944, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3584, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          )
        )
        (norm): Qwen2RMSNorm((3584,), eps=1e-06)
        (rotary_emb): Qwen2RotaryEmbedding()
      )
      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
    )
  )
)
[INFO:swift] model_parameter_info: PeftModelForCausalLM: 7635.8016M Params (20.1851M Trainable [0.2643%]), 0.0001M Buffers.
/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/trainers/mixin.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GRPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
The model is already on multiple devices. Skipping the move to device specified in `args`.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
[INFO:swift.hub.hub] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-7B-Instruct
[INFO:modelscope] Target directory already exists, skipping creation.
[INFO:swift] Loading the model using model_dir: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct
[INFO:swift] Setting torch_dtype: torch.bfloat16
[INFO:swift] model_kwargs: {'device_map': 'auto'}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.26s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]
[WARNING:swift] The requested device cuda:0 is also used for training. This may lead to unexpected behavior. It is recommended to use a dedicated device for vLLM.
[INFO:swift] Loading the model using model_dir: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[rank0]:[W210 07:48:38.775961177 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.09s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.08s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.91s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.99s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.99s/it]


Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|▏         | 1/67 [00:00<00:07,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:05, 10.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 5/67 [00:00<00:06,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|█         | 7/67 [00:00<00:04, 12.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:00<00:04, 13.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▋        | 11/67 [00:00<00:04, 13.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:01<00:03, 13.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:01<00:05,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:01<00:06,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:01<00:05,  9.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:01<00:04, 10.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:02<00:03, 11.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:02<00:03, 11.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:02<00:03, 12.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:02<00:02, 14.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:02<00:01, 17.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:02<00:01, 20.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:02<00:01, 22.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 42/67 [00:03<00:01, 21.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 45/67 [00:03<00:00, 23.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:03<00:00, 24.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:03<00:00, 26.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▎ | 56/67 [00:03<00:00, 28.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|████████▉ | 60/67 [00:03<00:00, 29.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 64/67 [00:03<00:00, 30.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 17.13it/s]
[INFO:swift] The logging file will be saved in: /qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/output/v15-20260210-074724/logging.jsonl
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/cli/rlhf.py", line 5, in <module>
[rank0]:     rlhf_main()
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/llm/train/rlhf.py", line 96, in rlhf_main
[rank0]:     return SwiftRLHF(args).main()
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/llm/base.py", line 46, in main
[rank0]:     result = self.run()
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/llm/train/sft.py", line 143, in run
[rank0]:     return self.train(trainer)
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/llm/train/sft.py", line 202, in train
[rank0]:     trainer.train(trainer.args.resume_from_checkpoint)
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/trainers/mixin.py", line 266, in train
[rank0]:     res = super().train(*args, **kwargs)
[rank0]:   File "/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/transformers/trainer.py", line 2375, in _inner_training_loop
[rank0]:     train_dataloader = self.get_train_dataloader()
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/trainers/mixin.py", line 343, in get_train_dataloader
[rank0]:     return super().get_train_dataloader()
[rank0]:   File "/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 725, in get_train_dataloader
[rank0]:     dataloader_params["sampler"] = self._get_train_sampler()
[rank0]:   File "/qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/trainers/mixin.py", line 337, in _get_train_sampler
[rank0]:     return super()._get_train_sampler()
[rank0]:   File "/qiuyeqing/tools/miniconda3/envs/prl/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 768, in _get_train_sampler
[rank0]:     shuffle=self.shuffle_dataset,
[rank0]: AttributeError: 'GRPOTrainer' object has no attribute 'shuffle_dataset'
[rank0]:[W210 07:50:52.889983681 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

ERROR conda.cli.main_run:execute(127): `conda run bash scripts/mr/mr_qwen_qwen.sh` failed. (See above for error)
run sh: `/qiuyeqing/tools/miniconda3/envs/prl/bin/python3.10 /qiuyeqing/llama_prl/PRL-REDO/PRL-addENV/swift/cli/rlhf.py --rlhf_type grpo --model Qwen/Qwen2.5-7B-Instruct --model_type qwen2_5 --dataset datasets/original/mr_train.jsonl --val_dataset datasets/original/mr_val.jsonl --reward_funcs accuracy format --torch_dtype bfloat16 --gradient_checkpointing_kwargs {"use_reentrant": false} --use_lmdeploy false --use_vllm true --vllm_gpu_memory_utilization 0.5 --train_type lora --lora_rank 8 --lora_alpha 32 --seed 5 --max_completion_length 1024 --num_train_epochs 10 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --learning_rate 1e-6 --gradient_accumulation_steps 1 --eval_steps 100 --save_steps 100 --save_total_limit 20 --max_length 2048 --output_dir output --warmup_ratio 0 --dataloader_num_workers 1 --dataset_num_proc 4 --num_generations 4 --temperature 0.9 --report_to wandb --logging_steps 5 --system examples/train/grpo/prompt.txt --log_completions true --num_iterations 1 --num_infer_workers 1`
⚙️  Running in WANDB offline mode
Downloading Model from https://www.modelscope.cn to directory: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct
Downloading Model from https://www.modelscope.cn to directory: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct
INFO 02-10 07:47:56 [__init__.py:216] Automatically detected platform cuda.
Downloading Model from https://www.modelscope.cn to directory: /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2.5-7B-Instruct
INFO 02-10 07:48:34 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
INFO 02-10 07:48:34 [__init__.py:1815] Using max model len 32768
INFO 02-10 07:48:35 [parallel.py:348] Disabling V1 multiprocessing for external launcher.
INFO 02-10 07:48:35 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 02-10 07:48:36 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 02-10 07:48:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 02-10 07:48:38 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
INFO 02-10 07:48:38 [gpu_model_runner.py:2338] Starting to load model /qiuyeqing/.cache/modelscope/models/Qwen/Qwen2___5-7B-Instruct...
INFO 02-10 07:48:39 [gpu_model_runner.py:2370] Loading model from scratch...
INFO 02-10 07:48:40 [cuda.py:362] Using Flash Attention backend on V1 engine.
INFO 02-10 07:48:56 [default_loader.py:268] Loading weights took 16.03 seconds
INFO 02-10 07:48:57 [gpu_model_runner.py:2392] Model loading took 14.2488 GiB and 17.523870 seconds
INFO 02-10 07:49:02 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/2051a50d92/rank_0_0/backbone for vLLM's torch.compile
INFO 02-10 07:49:02 [backends.py:550] Dynamo bytecode transform time: 4.58 s
INFO 02-10 07:49:06 [backends.py:194] Cache the graph for dynamic shape for later use
INFO 02-10 07:49:23 [backends.py:215] Compiling a graph for dynamic shape takes 20.81 s
INFO 02-10 07:49:25 [monitor.py:34] torch.compile takes 25.39 s in total
INFO 02-10 07:50:45 [gpu_worker.py:298] Available KV cache memory: 24.32 GiB
INFO 02-10 07:50:45 [kv_cache_utils.py:864] GPU KV cache size: 455,296 tokens
INFO 02-10 07:50:45 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 13.89x
INFO 02-10 07:50:50 [gpu_model_runner.py:3118] Graph capturing finished in 5 secs, took 0.48 GiB
INFO 02-10 07:50:50 [gpu_worker.py:391] Free memory on device (49.85/79.25 GiB) on startup. Desired GPU memory utilization is (0.5, 39.63 GiB). Actual usage is 14.25 GiB for weight, 1.04 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.48 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=25435443200` to fit into requested memory, or `--kv-cache-memory=36408659968` to fully utilize gpu memory. Current kv cache memory in use is 26108628992 bytes.
INFO 02-10 07:50:50 [core.py:218] init engine (profile, create kv cache, warmup model) took 112.84 seconds

