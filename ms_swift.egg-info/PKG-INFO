Metadata-Version: 2.4
Name: ms_swift
Version: 3.3.0.dev0
Summary: Swift: Scalable lightWeight Infrastructure for Fine-Tuning
Home-page: https://github.com/modelscope/swift
Author: DAMO ModelScope teams
Author-email: contact@modelscope.cn
License: Apache License 2.0
Keywords: python,petl,efficient tuners
Classifier: Development Status :: 4 - Beta
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.1
Requires-Dist: transformers>=4.45
Requires-Dist: accelerate>=0.30
Requires-Dist: datasets>=2.18
Requires-Dist: easse>=0.2.4
Requires-Dist: peft>=0.11
Requires-Dist: modelscope>=1.20
Requires-Dist: huggingface_hub>=0.24
Requires-Dist: safetensors>=0.4
Requires-Dist: numpy>=1.24
Requires-Dist: pandas>=2.0
Requires-Dist: tqdm>=4.66
Requires-Dist: packaging>=23.2
Requires-Dist: requests>=2.31
Requires-Dist: aiohttp>=3.9
Requires-Dist: matplotlib>=3.8
Requires-Dist: tensorboard>=2.14
Requires-Dist: dacite>=1.8
Requires-Dist: importlib-metadata>=6.8
Requires-Dist: einops>=0.7
Requires-Dist: Pillow>=10.0
Requires-Dist: psutil>=5.9
Requires-Dist: gradio>=4.0
Requires-Dist: fastapi>=0.110
Requires-Dist: uvicorn>=0.27
Requires-Dist: openai>=1.0
Requires-Dist: trl @ git+https://github.com/huggingface/trl.git
Requires-Dist: lmdeploy>=0.7.0
Requires-Dist: wandb>=0.17
Requires-Dist: rouge>=1.0.1
Requires-Dist: mosestokenizer>=1.2.1
Provides-Extra: eval
Requires-Dist: evalscope>=0.9.0; extra == "eval"
Provides-Extra: swanlab
Requires-Dist: swanlab>=0.4.0; extra == "swanlab"
Provides-Extra: seq-parallel
Requires-Dist: deepspeed>=0.14; platform_system != "Windows" and extra == "seq-parallel"
Provides-Extra: all
Requires-Dist: torch>=2.1; extra == "all"
Requires-Dist: transformers>=4.45; extra == "all"
Requires-Dist: accelerate>=0.30; extra == "all"
Requires-Dist: datasets>=2.18; extra == "all"
Requires-Dist: easse>=0.2.4; extra == "all"
Requires-Dist: peft>=0.11; extra == "all"
Requires-Dist: modelscope>=1.20; extra == "all"
Requires-Dist: huggingface_hub>=0.24; extra == "all"
Requires-Dist: safetensors>=0.4; extra == "all"
Requires-Dist: numpy>=1.24; extra == "all"
Requires-Dist: pandas>=2.0; extra == "all"
Requires-Dist: tqdm>=4.66; extra == "all"
Requires-Dist: packaging>=23.2; extra == "all"
Requires-Dist: requests>=2.31; extra == "all"
Requires-Dist: aiohttp>=3.9; extra == "all"
Requires-Dist: matplotlib>=3.8; extra == "all"
Requires-Dist: tensorboard>=2.14; extra == "all"
Requires-Dist: dacite>=1.8; extra == "all"
Requires-Dist: importlib-metadata>=6.8; extra == "all"
Requires-Dist: einops>=0.7; extra == "all"
Requires-Dist: Pillow>=10.0; extra == "all"
Requires-Dist: psutil>=5.9; extra == "all"
Requires-Dist: gradio>=4.0; extra == "all"
Requires-Dist: fastapi>=0.110; extra == "all"
Requires-Dist: uvicorn>=0.27; extra == "all"
Requires-Dist: openai>=1.0; extra == "all"
Requires-Dist: trl @ git+https://github.com/huggingface/trl.git ; extra == "all"
Requires-Dist: lmdeploy>=0.7.0; extra == "all"
Requires-Dist: wandb>=0.17; extra == "all"
Requires-Dist: rouge>=1.0.1; extra == "all"
Requires-Dist: mosestokenizer>=1.2.1; extra == "all"
Requires-Dist: evalscope>=0.9.0; extra == "all"
Requires-Dist: deepspeed>=0.14; platform_system != "Windows" and extra == "all"
Requires-Dist: swanlab>=0.4.0; extra == "all"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: summary

<div align="center">
<h1>PRL: Prompts from Reinforcement Learning</h1>

Pawe≈Ç Batorski, Adrian Kosmala, Paul Swoboda


[![arXiv](https://img.shields.io/badge/arXiv-2505.14412-red)](https://arxiv.org/abs/2505.14412)  

</div>

<div align="center">
  <img src="imgs/prl.png" alt="PRL Figure">
</div>

---

This repository contains the official implementation of the paper  
**[PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)**

**Abstract:**  
Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior.  
In this paper, we introduce **PRL (Prompts from Reinforcement Learning)**, a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization.

---

## ‚ú® Highlights

- üöÄ Outperforms prior prompt optimization methods like APE and EvoPrompt
- üß† Automatically generates **novel few-shot prompts** unseen during training
- üìà Strong gains across multiple NLP tasks:
  - **+2.58%** over APE and **+1.00%** over EvoPrompt (Classification)
  - **+4.32 ROUGE** over APE and **+2.12** over EvoPrompt (Summarization)
  - **+6.93 SARI** over APE and **+6.01** over EvoPrompt (Simplification)

---

## üõ†Ô∏è Installation & Setup

PRL is based on the [**ms-swift**](https://github.com/modelscope/ms-swift) framework.  
Please follow the environment setup instructions provided in that repository.

You also need to download the benchmark datasets (Classification, Summarization, Simplification) from  
üëâ [https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar](https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar)

---

## üß™ Benchmarks

### üî§ Classification

To train PRL on the MR dataset:
```bash
./scripts/mr/mr_qwen_qwen.sh
```

### üìö Summarization
To run PRL on summarization tasks:
```bash
./scripts/sum/sum_qwen_qwen.sh
```

### ‚úèÔ∏è Simplification
To evaluate PRL on simplification:
```bash
./scripts/sim/sim_qwen_qwen.sh
```

## Non-interactive Platform Run

For compute platforms, you can use the non-interactive launcher without changing training logic.

1. Prepare env vars (optional but recommended):
```bash
cp scripts/platform/platform.env.example scripts/platform/platform.env
set -a && source scripts/platform/platform.env && set +a
```

2. Run one task:
```bash
bash scripts/platform/run_prl_non_interactive.sh mr
# or: sum / sim
```

The launcher reuses original scripts under `scripts/<task>/`, auto-loads `scripts/platform/platform.env` when present,
and runs `scripts/platform/preflight_prl.sh` before training to fail fast on missing dependencies/GPU/files.

## üìÑ Citation

If you find our work useful, please consider citing:

```bibtex
@article{batorski2025prl,
  title     = {PRL: Prompts from Reinforcement Learning},
  author    = {Batorski, Pawe{\l} and Kosmala, Adrian and Swoboda, Paul},
  journal   = {arXiv preprint arXiv:2505.14412},
  year      = {2025}
}
```

## üôè Acknowledgments

[![ms-swift](https://img.shields.io/badge/Base%20Code-ms--swift-blue)](https://github.com/modelscope/ms-swift)
[![APE](https://img.shields.io/badge/Inspired%20by-APE-red)](https://arxiv.org/abs/2211.01910)
[![EvoPrompt](https://img.shields.io/badge/Inspired%20by-EvoPrompt-orange)](https://arxiv.org/abs/2309.08532)
[![APO](https://img.shields.io/badge/Inspired%20by-APO-yellow)](https://arxiv.org/abs/2305.03495)

This work builds on the [**ms-swift**](https://github.com/modelscope/ms-swift) framework.  
We thank the authors of [**APE**](https://arxiv.org/abs/2211.01910), [**EvoPrompt**](https://arxiv.org/abs/2309.08532), and [**APO**](https://arxiv.org/abs/2305.03495) for their inspiring contributions to the field of automated prompt generation.

